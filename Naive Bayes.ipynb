{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dabefc-6081-4708-bf6f-5da6be4a0ed6",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\"> Naive Bayes </span> \n",
    "\n",
    "Naive Bayes is a simple classification algorithm based on Bayes' theorem. It calculates the likelihood of an event happening given prior knowledge of other events. It performs particularly well on high-dimensional data.\n",
    "\n",
    "<b> Pros and Cons for Naive Bayes </b>\n",
    "\n",
    "Pros:\n",
    "- Naive Bayes can be used on small amounts of training data.\n",
    "- It can handle continuous and discrete data.\n",
    "- It is simple and fast.\n",
    "- Can be used for both binary and multi-class classification problems.\n",
    "\n",
    "Cons:\n",
    "- the assumption of linear independence is probably not particularly accurate\n",
    "- If there is a categorical value in the test data set that doesn't appear in the training set, it will be allocated a probability of 0.\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Bayes' Theorem </span> \n",
    "\n",
    "Baye's theorem looks at the probability of a class, given a feature vector X.\n",
    "\n",
    "P(C|X) = P(X|C)xP(C)/P(X)\n",
    "\n",
    "The 'naive' part is assuming the features are independent, i.e. the probability that A and B occur simultaneously, is equal to A occuring and B occuring i.e. P(A union B) = P(A)P(B). This means they're also conditionally independent:\n",
    "\n",
    "P(X1,X2,....Xn|C) = P(X1|C)P(X2|C)......P(Xn|C)\n",
    "\n",
    "Naive Bayes works well for text classification because text datasets typically have high dimensionality. Text classification involves relatively simple relationship and is less likely to violate the assumption of naivety. Naive Bayes works well with high dimensionality because of th\n",
    "\n",
    "<b> Types of Naive Bayes: </b>\n",
    "\n",
    "- If the features are continuous and are assumed to follow a normal distribution, you use a Gaussian Naive Bayes. You can normalise the data prior to using this.\n",
    "- If the features are discrete/categorical data like text then you can use multinomial Naive Bayes (i.e. generalisation of a binomial where there's k experiments and two classes, to multiple classes)\n",
    "- If the features are binary, can use the Bernoulli Naive Bayes.\n",
    "\n",
    "Step 1) For each input feature outcome, calculate the frequency of occurences of that outcome relative to the no. of examples to get P(X), then calculate the probability of each class P(C). Then we can calculate the likelihood P(X|C) based on the distribution assumed (Gaussian, Multinomial, Bernoulli). \n",
    "\n",
    "Step 2) Laplace Smoothing \n",
    "\n",
    "This is a technique for smoothing categorical data. A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Hence, no probability will be zero or too small. This prevents the whole calculation from being 0 (and also division by 0 causing errors). It also prevents underflow, which is when very small probabilities cause you to reach the limit of numerical precision and get rounded down to 0. \n",
    "\n",
    "Step 3) Calculate the posterior probability P(Câˆ£X) via Baye's theorem. \n",
    "\n",
    "Step 4) The class with the highest posterior probability is the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb03e8f-fec3-4b58-8fb5-cf51a40f3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Suppose we already have X_train and Y_train\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "naive_bayes_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
