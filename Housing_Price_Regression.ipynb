{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f623c1-802a-4880-912c-8f3a91f3d17a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\"> House Prices Regression </span> \n",
    "\n",
    "We will be predicting house prices using XG Boost.\n",
    "\n",
    "Key skills covered: one hot encoding, normalisation methods, XGboost algorithm, Principle Component Analysis, \n",
    "    \n",
    "<span style=\"font-size: 20px;\"> Dataset </span> \n",
    "\n",
    "There are 79 input features, 1460 data points.\n",
    "\n",
    "The following variables will be dropped due to a high prevalence of N/A values:\n",
    "- Alley\n",
    "- MiscFeature\n",
    "- Fence\n",
    "- PoolOC\n",
    "- PoolArea\n",
    "- MiscVal\n",
    "- GarageYrBlt\n",
    "\n",
    "Other preprocessing tasks:\n",
    "\n",
    "- LotFrontage: Linear feet of street connected to property. Lots of N/A values. I'll set them to 0 if N/A.\n",
    "- OpenPorchSF: Open porch area in square feet. CREATE A NEW COL CALLED SUM_PORCH_AREA AND COMBINE THIS COL WITH EnclosedPorch, 3SsnPorch and ScreenPorch\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Strategy </span> \n",
    "\n",
    "I'm going to use PCA for dimensionality reduction, it isn't strictly required as we are using an ensemble method called XG Boost which can cope with a high number of features.\n",
    "\n",
    "Before I do that I'll need to normalise the data.\n",
    "\n",
    "<b> Data Normalisation </b>\n",
    "\n",
    "There are three typical methods for Data Normalisation:\n",
    "- Min-Max scaling scales features to a specific range, usually [0, 1]. X_normalized = (X - X_min) / (X_max - X_min)\n",
    "- Standardisation is just the usual normalisation. It's good for when the data isn't currently a normal distribution. X_standardized = (X - X_mean) / X_std\n",
    "- Robust scaling scales features using the median and interquartile range (IQR) instead of the mean and standard deviation. X_robust = (X - X_median) / IQR. This is good for when there are definitely outliers. Why? Because outliers affect mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8e102cae-7546-4200-9335-2667ac28eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path_train = \"/home/frances/Documents/ML preparation/House_Prices_Regression/train.csv\"\n",
    "path_test = \"/home/frances/Documents/ML preparation/House_Prices_Regression/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(path_train)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "categorical_vars = [\n",
    "    \"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\",\n",
    "    \"LandSlope\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\",\n",
    "    \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"ExterQual\", \"ExterCond\", \"Foundation\",\n",
    "    \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"Heating\", \"HeatingQC\",\n",
    "    \"CentralAir\", \"Electrical\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\",\n",
    "    \"GarageQual\", \"GarageCond\", \"PavedDrive\", \"SaleType\", \"SaleCondition\", \n",
    "]\n",
    "\n",
    "numerical_vars = [\n",
    "    \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"YearBuilt\", \"YearRemodAdd\",\n",
    "    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "    \"GrLivArea\", \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\",\n",
    "    \"TotRmsAbvGrd\", \"Fireplaces\", \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"MoSold\", \"YrSold\"\n",
    "]\n",
    "\n",
    "# we split the data into categorical and numerical variables\n",
    "\n",
    "train_categorical = df_train[categorical_vars]\n",
    "train_numerical = df_train[numerical_vars]\n",
    "test_categorical = df_test[categorical_vars]\n",
    "test_numerical = df_test[numerical_vars]\n",
    "\n",
    "# we encode the variables \n",
    "train_encoded = pd.get_dummies(train_categorical, columns=categorical_vars)\n",
    "test_encoded = pd.get_dummies(test_categorical, columns=categorical_vars)\n",
    "\n",
    "# we initialise the scaler to normalise the numerical data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "test_standardised = scaler.fit_transform(test_numerical) # this returns a numpy array. We don't technically have to convert it back as the model will accept numpy arrays, but for consistency we will.\n",
    "test_standardised = pd.DataFrame(test_standardised, columns=numerical_vars)\n",
    "train_standardised = scaler.fit_transform(train_numerical) # this returns a numpy array. We don't technically have to convert it back as the model will accept numpy arrays, but for consistency we will.\n",
    "train_standardised = pd.DataFrame(train_standardised, columns=numerical_vars)\n",
    "\n",
    "# we join the numerical and categorical data up once more\n",
    "X_train = pd.concat([train_standardised, train_encoded],axis=1)\n",
    "X_test = pd.concat([test_standardised, test_encoded],axis=1)\n",
    "\n",
    "X_train['LotFrontage'] = X_train['LotFrontage'].fillna(0)\n",
    "X_train['SUM_PORCH_AREA'] = X_train['OpenPorchSF'] + X_train['EnclosedPorch'] + X_train['3SsnPorch'] + X_train['ScreenPorch']\n",
    "X_test['LotFrontage'] = X_test['LotFrontage'].fillna(0)\n",
    "X_test['SUM_PORCH_AREA'] = X_test['OpenPorchSF'] + X_test['EnclosedPorch'] + X_test['3SsnPorch'] + X_test['ScreenPorch']\n",
    "\n",
    "X_train.drop(['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'],axis=1)\n",
    "X_test.drop(['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'],axis=1)\n",
    "\n",
    "temp = pd.concat([X_train, df_train['SalePrice']],axis=1)\n",
    "X_train = temp.dropna()\n",
    "X_test = X_test.dropna()\n",
    "X_train = temp.drop(['SalePrice'],axis=1)\n",
    "Y_train = temp['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fbcf7-c3f0-4880-bdf3-f47f97098fcf",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> Theory behind Principle Component Analysis </span> \n",
    "\n",
    "- The data first needs to be processed and standardised.\n",
    "- The covariance matrix is calculated. The covariance between two features i and j with n data points is computed as below.\n",
    "\n",
    "Computation: Take row 1, take feature i in that row, minus the average value for the column. Do the same for the other feature, then multiply them. Do that for all rows, adding them up, then divide by n-1. The fact that it's n-1 and not n is known as Bessel's correction. It makes the covariance an unbiased estimator (statistics stuff).\n",
    "\n",
    "This tells us how much one variable varies with another. Had we not normalised the variables, it wouldn't be particularly useful\n",
    "\n",
    "- Then we compute eigenvalues of the covariance matrix, sorting and selecting the k largest.\n",
    "\n",
    "Eigenvectors represent directions in the original feature space. These eigenvectors (principle components) point in the directions of maximum variance (spread) in the feature space. Note, a feature vector would be a vector where each entry is a numerical value corresponding to a given feature. The feature space is like a set of all possible combinations of those values. If you tried to flatten a dimension where there's high variance, you'd be removing a lot of datapoints. but if you flatten a dimension with low variance, you don't remove much data. This means that PC's help us find the most important features, and allow us to reduce dimensions with too much loss of data.\n",
    "\n",
    "<b> Methods to choose k </b>\n",
    "\n",
    "There are a few different methods you can use to determine the value of k. We'll focus on one in particular, the Explained Variance Ratio. Suppose we take the cumulatively keep adding eigenvalues together, dividing the total sum of eigenvectors, and plot this as we add more and more. This is known as the explained variance ratio. We'd like to get to the point where we capture 95% or even 99% of the variance.\n",
    "\n",
    "- Then we project the original data onto the subspace defined by the select PCs. \n",
    "\n",
    "Note that there is another dimensionality reduction method which is typically used for classification models, called Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8a73f2d2-1c26-4056-b131-1b5992a83ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "(1460, 286)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# initialise PCA\n",
    "pca = PCA()\n",
    "\n",
    "# fit the pca to the training data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Calculate the explained variance ratio and cumulative explained variance ratio. This uses the fitted model.\n",
    "exp = pca.explained_variance_ratio_\n",
    "cum = np.cumsum(exp) # this is an array of floats \n",
    "\n",
    "# Note that cum >= 0.95 creates a boolean array where each cumulative explained variance is checked to see if it's higher than 0.95\n",
    "# We then use argmax to find the index, which gives us k\n",
    "\n",
    "num_components = np.argmax(cum >= 0.95) + 1\n",
    "\n",
    "print(num_components)\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f84fc-94b6-4f9e-a962-1d9dc83894a7",
   "metadata": {},
   "source": [
    "It turns out that out of 286 features, 74 of them are sufficient to explain 95% of the variance.\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Theory behind Gradient Boosting Regression </span> \n",
    "\n",
    "1) The model initialises the prediction vector y_pred e.g. as the average value found in the training data for regression, or the natural logarithm of the odds ratio (Probability of Event/Probability of Non-Event) for classification.\n",
    "\n",
    "2) The model calculates the residuals (errors) and uses these as new y train values, with which to fit a decision tree. \n",
    "\n",
    "What this does is it generates predicted residual (error) values, which we then add to y_pred, so we have y_pred + res_pred. Now we repeat this iteratively, with weak decision trees (3/4/5 layers). These are called 'weak learners'. The number of times we iterate (the number of weak learners) is a hyperparameter.\n",
    "\n",
    "The decision tree predicts these values on the basis of minimising the objective function, which is the negative log loss for classification and the mean squared error for regression.\n",
    "\n",
    "As with Random Forests, XGBoost does incorporate randomness in feature selection and data subsampling, but this randomness is not a central aspect of the boosting process itself. Instead, it's introduced as a regularization technique to prevent overfitting and improve the diversity of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "30fbda96-60dc-4a5c-92d9-5f67fe35d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.9995733051136423\n",
      "mse: 2691074.4387079733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Fit PCA on training data\n",
    "pca_reduced = PCA(74) # initialises a PCA which knows to keep 74 components\n",
    "\n",
    "pca_reduced.fit(X_train) # calculates principle components\n",
    "\n",
    "X_train_pca = pca_reduced.fit_transform(X_train) # this transforms the training set using the principle components\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "#X_train_pca, X_eval_pca, Y_train, Y_eval = train_test_split(X_train_pca, Y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_train, Y_train) # Note that this is the R^2. It represents the variance that the model captures. It's a value between 0 and 1. Higher values indiciate a better fit.\n",
    "print(\"Model Score:\", score)\n",
    "\n",
    "# Assuming you have actual target values (Y_actual) and predicted values (Y_pred)\n",
    "n = len(Y_train)\n",
    "mse = np.sum((Y_train - y_train_pred)**2) / n\n",
    "\n",
    "print(\"mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "27c1d2fc-41c7-4eab-bde8-468a59da0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best Parameters: {'subsample': 0.6, 'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 0.9}\n",
      "Best Score: -795783707.0022014\n"
     ]
    }
   ],
   "source": [
    "# A score of 99% for an R^2 value indicates a lot of overfitting. Let's see if we can tune the hyperparameters to make it better.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(100, 1001, 100),# no. of weak learners/iterations\n",
    "    'max_depth': np.arange(3, 10), # how deep the trees can go\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], # a scaling factor which scales each weak learners input\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # fraction of sample used for a tree\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # fraction of features\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4], # regularisation parameter, \n",
    "}\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_squared_error',  # Use appropriate metric\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Print the best parameters and corresponding score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b621ea-a026-4084-969f-9c1d217bd07d",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> Next steps </span> \n",
    "\n",
    "So with or without dimensionality reduction, there's still high overfitting and large error. Not sure what happened there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
