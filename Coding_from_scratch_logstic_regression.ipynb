{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a05ce9-0fbf-4dab-b43e-6a1fa69067ec",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\"> Coding from scratch - Logistic Regression </span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "926c50b8-d896-4e96-862c-79da74367b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "path_train = \"/home/frances/Documents/Getting a JOB/ML preparation/Titanic_Classification/train.csv\"\n",
    "path_test = \"/home/frances/Documents/Getting a JOB/ML preparation/Titanic_Classification/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(path_train)\n",
    "test_data = pd.read_csv(path_test)\n",
    "\n",
    "# Create a new instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to 'Sex' and 'Embarked' columns in training data\n",
    "train_data['Sex'] = label_encoder.fit_transform(train_data['Sex'])\n",
    "train_data['Embarked'] = label_encoder.fit_transform(train_data['Embarked'])\n",
    "test_data['Sex'] = label_encoder.fit_transform(test_data['Sex'])\n",
    "test_data['Embarked'] = label_encoder.fit_transform(test_data['Embarked'])\n",
    "\n",
    "X_train = train_data.drop(['Name', 'Parch', 'Ticket','Cabin'],axis=1) # remove unnecessary columns\n",
    "X_test = test_data.drop(['Name', 'Parch', 'Ticket','Cabin'],axis=1) # remove unnecessary columns\n",
    "\n",
    "# There are 177 na values in the train set, and 87 na values in the test data, which is very high so I don't want to drop them.\n",
    "\n",
    "cols_with_missing_data = ['Age','Fare']\n",
    "\n",
    "# There are 891 samples, sqrt(891) = 30 so I'll use 30 nearest neighbours.\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=30)\n",
    "\n",
    "# Perform imputation on training data\n",
    "imputed_train_data = X_train.copy()\n",
    "imputed_train_data[cols_with_missing_data] = knn_imputer.fit_transform(X_train[cols_with_missing_data])\n",
    "\n",
    "# Perform imputation on test data\n",
    "imputed_test_data = X_test.copy()\n",
    "imputed_test_data[cols_with_missing_data] = knn_imputer.transform(X_test[cols_with_missing_data])\n",
    "\n",
    "X_train = imputed_train_data\n",
    "X_test = imputed_test_data\n",
    "\n",
    "Y_train = X_train['Survived']\n",
    "X_train = X_train.drop(['Survived'],axis=1) # remove unnecessary columns\n",
    "\n",
    "X_train['SibSp'] = np.log(X_train['SibSp'] + 1)  # Adding 1 to handle zero values\n",
    "X_train['Fare'] = np.log(X_train['Fare'] + 1)\n",
    "X_test['SibSp'] = np.log(X_test['SibSp'] + 1) \n",
    "X_test['Fare'] = np.log(X_test['Fare'] + 1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9920986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000 Loss: 0.46133930067188184\n",
      "Iteration: 2000 Loss: 0.4555242863858422\n",
      "Iteration: 3000 Loss: 0.4533087758205236\n",
      "Iteration: 4000 Loss: 0.4520750259361438\n",
      "Iteration: 5000 Loss: 0.45131242069683986\n",
      "Iteration: 6000 Loss: 0.4508164530324913\n",
      "Iteration: 7000 Loss: 0.4504812585822373\n",
      "Iteration: 8000 Loss: 0.4502466594752506\n",
      "Iteration: 9000 Loss: 0.45007696460087393\n",
      "Iteration: 10000 Loss: 0.44995042365925475\n",
      "Iteration: 11000 Loss: 0.4498534729736351\n",
      "Iteration: 12000 Loss: 0.4497774570562145\n",
      "Iteration: 13000 Loss: 0.4497167143452497\n",
      "Iteration: 14000 Loss: 0.44966744046737134\n",
      "Iteration: 15000 Loss: 0.4496270039678818\n",
      "Iteration: 16000 Loss: 0.44959352923495854\n",
      "Iteration: 17000 Loss: 0.4495656390367477\n",
      "Iteration: 18000 Loss: 0.4495422933573664\n",
      "Iteration: 19000 Loss: 0.44952268687844116\n",
      "Iteration: 20000 Loss: 0.4495061825199601\n",
      "Iteration: 21000 Loss: 0.4494922673900487\n",
      "Iteration: 22000 Loss: 0.44948052283550455\n",
      "Iteration: 23000 Loss: 0.4494706035000133\n",
      "Iteration: 24000 Loss: 0.44946222224301297\n",
      "Iteration: 25000 Loss: 0.449455138956293\n",
      "Iteration: 26000 Loss: 0.4494491520397391\n",
      "Iteration: 27000 Loss: 0.4494440917432353\n",
      "Iteration: 28000 Loss: 0.44943981485775253\n",
      "Iteration: 29000 Loss: 0.44943620041108673\n",
      "Iteration: 30000 Loss: 0.4494331461326272\n",
      "Iteration: 31000 Loss: 0.44943056552131666\n",
      "Iteration: 32000 Loss: 0.4494283853964903\n",
      "Iteration: 33000 Loss: 0.44942654384165126\n",
      "Iteration: 34000 Loss: 0.44942498847204826\n",
      "Iteration: 35000 Loss: 0.44942367497159114\n",
      "Iteration: 36000 Loss: 0.4494225658553285\n",
      "Iteration: 37000 Loss: 0.44942162942172126\n",
      "Iteration: 38000 Loss: 0.4494208388651379\n",
      "Iteration: 39000 Loss: 0.4494201715238923\n",
      "Iteration: 40000 Loss: 0.44941960824309785\n",
      "Iteration: 41000 Loss: 0.44941913283485635\n",
      "Iteration: 42000 Loss: 0.4494187316209899\n",
      "Iteration: 43000 Loss: 0.4494183930457831\n",
      "Iteration: 44000 Loss: 0.4494181073480964\n",
      "Iteration: 45000 Loss: 0.4494178662838241\n",
      "Iteration: 46000 Loss: 0.44941766289102564\n",
      "Iteration: 47000 Loss: 0.4494174912912242\n",
      "Iteration: 48000 Loss: 0.4494173465213412\n",
      "Iteration: 49000 Loss: 0.44941722439158\n",
      "Iteration: 50000 Loss: 0.4494171213652768\n",
      "Iteration: 51000 Loss: 0.44941703445734527\n",
      "Iteration: 52000 Loss: 0.4494169611484538\n",
      "Iteration: 53000 Loss: 0.4494168993125115\n",
      "Iteration: 54000 Loss: 0.44941684715540775\n",
      "Iteration: 55000 Loss: 0.4494168031632692\n",
      "Iteration: 56000 Loss: 0.4494167660587596\n",
      "Iteration: 57000 Loss: 0.44941673476417765\n",
      "Iteration: 58000 Loss: 0.44941670837030007\n",
      "Iteration: 59000 Loss: 0.4494166861100766\n",
      "Iteration: 60000 Loss: 0.4494166673364254\n",
      "Iteration: 61000 Loss: 0.44941665150348886\n",
      "Iteration: 62000 Loss: 0.44941663815081584\n",
      "Iteration: 63000 Loss: 0.44941662689000983\n",
      "Iteration: 64000 Loss: 0.4494166173934633\n",
      "Iteration: 65000 Loss: 0.44941660938485045\n",
      "Iteration: 66000 Loss: 0.4494166026311054\n",
      "Iteration: 67000 Loss: 0.44941659693565444\n",
      "Iteration: 68000 Loss: 0.44941659213270513\n",
      "Iteration: 69000 Loss: 0.44941658808242935\n",
      "Iteration: 70000 Loss: 0.44941658466689877\n",
      "Iteration: 71000 Loss: 0.4494165817866566\n",
      "Iteration: 72000 Loss: 0.44941657935782586\n",
      "Iteration: 73000 Loss: 0.4494165773096691\n",
      "Iteration: 74000 Loss: 0.4494165755825313\n",
      "Iteration: 75000 Loss: 0.4494165741261041\n",
      "Iteration: 76000 Loss: 0.44941657289796144\n",
      "Iteration: 77000 Loss: 0.4494165718623253\n",
      "Iteration: 78000 Loss: 0.4494165709890242\n",
      "Iteration: 79000 Loss: 0.4494165702526145\n",
      "Iteration: 80000 Loss: 0.44941656963164023\n",
      "Iteration: 81000 Loss: 0.44941656910800787\n",
      "Iteration: 82000 Loss: 0.4494165686664593\n",
      "Iteration: 83000 Loss: 0.4494165682941282\n",
      "Iteration: 84000 Loss: 0.44941656798016455\n",
      "Iteration: 85000 Loss: 0.44941656771541916\n",
      "Iteration: 86000 Loss: 0.4494165674921766\n",
      "Iteration: 87000 Loss: 0.44941656730393115\n",
      "Iteration: 88000 Loss: 0.4494165671451966\n",
      "Iteration: 89000 Loss: 0.4494165670113468\n",
      "Iteration: 90000 Loss: 0.44941656689848064\n",
      "Iteration: 91000 Loss: 0.4494165668033086\n",
      "Iteration: 92000 Loss: 0.44941656672305696\n",
      "Iteration: 93000 Loss: 0.44941656665538654\n",
      "Iteration: 94000 Loss: 0.4494165665983251\n",
      "Iteration: 95000 Loss: 0.44941656655020945\n",
      "Iteration: 96000 Loss: 0.4494165665096371\n",
      "Iteration: 97000 Loss: 0.4494165664754256\n",
      "Iteration: 98000 Loss: 0.4494165664465775\n",
      "Iteration: 99000 Loss: 0.4494165664222523\n",
      "Iteration: 100000 Loss: 0.44941656640174066\n",
      "Iteration: 101000 Loss: 0.44941656638444477\n",
      "Iteration: 102000 Loss: 0.44941656636986044\n",
      "Iteration: 103000 Loss: 0.4494165663575627\n",
      "Iteration: 104000 Loss: 0.44941656634719296\n",
      "Iteration: 105000 Loss: 0.449416566338449\n",
      "Iteration: 106000 Loss: 0.44941656633107585\n",
      "Iteration: 107000 Loss: 0.44941656632485866\n",
      "Iteration: 108000 Loss: 0.44941656631961624\n",
      "Iteration: 109000 Loss: 0.4494165663151958\n",
      "Iteration: 110000 Loss: 0.4494165663114683\n",
      "Iteration: 111000 Loss: 0.44941656630832516\n",
      "Iteration: 112000 Loss: 0.44941656630567484\n",
      "Iteration: 113000 Loss: 0.44941656630344\n",
      "Iteration: 114000 Loss: 0.4494165663015557\n",
      "Iteration: 115000 Loss: 0.4494165662999667\n",
      "Iteration: 116000 Loss: 0.4494165662986268\n",
      "Iteration: 117000 Loss: 0.44941656629749704\n",
      "Iteration: 118000 Loss: 0.4494165662965444\n",
      "Iteration: 119000 Loss: 0.4494165662957411\n",
      "Iteration: 120000 Loss: 0.4494165662950637\n",
      "Iteration: 121000 Loss: 0.44941656629449256\n",
      "Iteration: 122000 Loss: 0.449416566294011\n",
      "Iteration: 123000 Loss: 0.4494165662936048\n",
      "Iteration: 124000 Loss: 0.44941656629326243\n",
      "Iteration: 125000 Loss: 0.44941656629297366\n",
      "Iteration: 126000 Loss: 0.44941656629273014\n",
      "Iteration: 127000 Loss: 0.4494165662925248\n",
      "Iteration: 128000 Loss: 0.4494165662923517\n",
      "Iteration: 129000 Loss: 0.4494165662922058\n",
      "Iteration: 130000 Loss: 0.4494165662920827\n",
      "Iteration: 131000 Loss: 0.4494165662919789\n",
      "Iteration: 132000 Loss: 0.4494165662918914\n",
      "Iteration: 133000 Loss: 0.44941656629181753\n",
      "Iteration: 134000 Loss: 0.44941656629175536\n",
      "Iteration: 135000 Loss: 0.4494165662917029\n",
      "Iteration: 136000 Loss: 0.4494165662916586\n",
      "Iteration: 137000 Loss: 0.4494165662916214\n",
      "Iteration: 138000 Loss: 0.44941656629158994\n",
      "Iteration: 139000 Loss: 0.44941656629156335\n",
      "Iteration: 140000 Loss: 0.449416566291541\n",
      "Iteration: 141000 Loss: 0.44941656629152216\n",
      "Iteration: 142000 Loss: 0.4494165662915063\n",
      "Iteration: 143000 Loss: 0.44941656629149285\n",
      "Iteration: 144000 Loss: 0.4494165662914815\n",
      "Iteration: 145000 Loss: 0.44941656629147203\n",
      "Iteration: 146000 Loss: 0.449416566291464\n",
      "Iteration: 147000 Loss: 0.44941656629145715\n",
      "Iteration: 148000 Loss: 0.44941656629145144\n",
      "Iteration: 149000 Loss: 0.44941656629144655\n",
      "Iteration: 150000 Loss: 0.44941656629144255\n",
      "Iteration: 151000 Loss: 0.4494165662914391\n",
      "Iteration: 152000 Loss: 0.4494165662914362\n",
      "Iteration: 153000 Loss: 0.4494165662914338\n",
      "Iteration: 154000 Loss: 0.44941656629143173\n",
      "Iteration: 155000 Loss: 0.44941656629142995\n",
      "Iteration: 156000 Loss: 0.4494165662914285\n",
      "Iteration: 157000 Loss: 0.4494165662914274\n",
      "Iteration: 158000 Loss: 0.4494165662914263\n",
      "Iteration: 159000 Loss: 0.44941656629142546\n",
      "Iteration: 160000 Loss: 0.4494165662914247\n",
      "Iteration: 161000 Loss: 0.449416566291424\n",
      "Iteration: 162000 Loss: 0.44941656629142346\n",
      "Iteration: 163000 Loss: 0.44941656629142307\n",
      "Iteration: 164000 Loss: 0.4494165662914227\n",
      "Iteration: 165000 Loss: 0.44941656629142235\n",
      "Iteration: 166000 Loss: 0.4494165662914221\n",
      "Iteration: 167000 Loss: 0.4494165662914219\n",
      "Iteration: 168000 Loss: 0.44941656629142174\n",
      "Iteration: 169000 Loss: 0.44941656629142157\n",
      "Iteration: 170000 Loss: 0.4494165662914214\n",
      "Iteration: 171000 Loss: 0.44941656629142135\n",
      "Iteration: 172000 Loss: 0.4494165662914212\n",
      "Iteration: 173000 Loss: 0.44941656629142107\n",
      "Iteration: 174000 Loss: 0.449416566291421\n",
      "Iteration: 175000 Loss: 0.449416566291421\n",
      "Iteration: 176000 Loss: 0.4494165662914209\n",
      "Iteration: 177000 Loss: 0.4494165662914209\n",
      "Iteration: 178000 Loss: 0.44941656629142085\n",
      "Iteration: 179000 Loss: 0.44941656629142085\n",
      "Iteration: 180000 Loss: 0.44941656629142085\n",
      "Iteration: 181000 Loss: 0.44941656629142085\n",
      "Iteration: 182000 Loss: 0.44941656629142074\n",
      "Iteration: 183000 Loss: 0.4494165662914207\n",
      "Iteration: 184000 Loss: 0.44941656629142074\n",
      "Iteration: 185000 Loss: 0.44941656629142074\n",
      "Iteration: 186000 Loss: 0.4494165662914207\n",
      "Iteration: 187000 Loss: 0.4494165662914207\n",
      "Iteration: 188000 Loss: 0.4494165662914207\n",
      "Iteration: 189000 Loss: 0.4494165662914207\n",
      "Iteration: 190000 Loss: 0.44941656629142074\n",
      "Iteration: 191000 Loss: 0.4494165662914207\n",
      "Iteration: 192000 Loss: 0.44941656629142074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 193000 Loss: 0.4494165662914207\n",
      "Iteration: 194000 Loss: 0.4494165662914207\n",
      "Iteration: 195000 Loss: 0.4494165662914207\n",
      "Iteration: 196000 Loss: 0.4494165662914207\n",
      "Iteration: 197000 Loss: 0.4494165662914207\n",
      "Iteration: 198000 Loss: 0.4494165662914207\n",
      "Iteration: 199000 Loss: 0.4494165662914207\n",
      "Iteration: 200000 Loss: 0.4494165662914207\n",
      "Iteration: 201000 Loss: 0.4494165662914207\n",
      "Iteration: 202000 Loss: 0.4494165662914207\n",
      "Iteration: 203000 Loss: 0.4494165662914207\n",
      "Iteration: 204000 Loss: 0.4494165662914207\n",
      "Iteration: 205000 Loss: 0.4494165662914207\n",
      "Iteration: 206000 Loss: 0.4494165662914207\n",
      "Iteration: 207000 Loss: 0.4494165662914207\n",
      "Iteration: 208000 Loss: 0.4494165662914207\n",
      "Iteration: 209000 Loss: 0.4494165662914207\n",
      "Iteration: 210000 Loss: 0.4494165662914207\n",
      "Iteration: 211000 Loss: 0.4494165662914207\n",
      "Iteration: 212000 Loss: 0.4494165662914207\n",
      "Iteration: 213000 Loss: 0.4494165662914207\n",
      "Iteration: 214000 Loss: 0.4494165662914207\n",
      "Iteration: 215000 Loss: 0.4494165662914207\n",
      "Iteration: 216000 Loss: 0.4494165662914207\n",
      "Iteration: 217000 Loss: 0.4494165662914207\n",
      "Iteration: 218000 Loss: 0.4494165662914207\n",
      "Iteration: 219000 Loss: 0.4494165662914207\n",
      "Iteration: 220000 Loss: 0.4494165662914207\n",
      "Iteration: 221000 Loss: 0.4494165662914207\n",
      "Iteration: 222000 Loss: 0.4494165662914207\n",
      "Iteration: 223000 Loss: 0.4494165662914207\n",
      "Iteration: 224000 Loss: 0.4494165662914207\n",
      "Iteration: 225000 Loss: 0.4494165662914207\n",
      "Iteration: 226000 Loss: 0.4494165662914207\n",
      "Iteration: 227000 Loss: 0.4494165662914207\n",
      "Iteration: 228000 Loss: 0.4494165662914207\n",
      "Iteration: 229000 Loss: 0.4494165662914207\n",
      "Iteration: 230000 Loss: 0.4494165662914207\n",
      "Iteration: 231000 Loss: 0.4494165662914207\n",
      "Iteration: 232000 Loss: 0.4494165662914207\n",
      "Iteration: 233000 Loss: 0.4494165662914207\n",
      "Iteration: 234000 Loss: 0.4494165662914207\n",
      "Iteration: 235000 Loss: 0.4494165662914207\n",
      "Iteration: 236000 Loss: 0.4494165662914207\n",
      "Iteration: 237000 Loss: 0.4494165662914207\n",
      "Iteration: 238000 Loss: 0.4494165662914207\n",
      "Iteration: 239000 Loss: 0.4494165662914207\n",
      "Iteration: 240000 Loss: 0.4494165662914207\n",
      "Iteration: 241000 Loss: 0.4494165662914207\n",
      "Iteration: 242000 Loss: 0.44941656629142074\n",
      "Iteration: 243000 Loss: 0.4494165662914207\n",
      "Iteration: 244000 Loss: 0.4494165662914207\n",
      "Iteration: 245000 Loss: 0.4494165662914207\n",
      "Iteration: 246000 Loss: 0.4494165662914207\n",
      "Iteration: 247000 Loss: 0.4494165662914207\n",
      "Iteration: 248000 Loss: 0.4494165662914207\n",
      "Iteration: 249000 Loss: 0.4494165662914207\n",
      "Iteration: 250000 Loss: 0.4494165662914207\n",
      "Iteration: 251000 Loss: 0.4494165662914207\n",
      "Iteration: 252000 Loss: 0.4494165662914207\n",
      "Iteration: 253000 Loss: 0.4494165662914207\n",
      "Iteration: 254000 Loss: 0.4494165662914207\n",
      "Iteration: 255000 Loss: 0.4494165662914207\n",
      "Iteration: 256000 Loss: 0.4494165662914207\n",
      "Iteration: 257000 Loss: 0.4494165662914207\n",
      "Iteration: 258000 Loss: 0.4494165662914207\n",
      "Iteration: 259000 Loss: 0.4494165662914207\n",
      "Iteration: 260000 Loss: 0.4494165662914207\n",
      "Iteration: 261000 Loss: 0.4494165662914207\n",
      "Iteration: 262000 Loss: 0.4494165662914207\n",
      "Iteration: 263000 Loss: 0.4494165662914207\n",
      "Iteration: 264000 Loss: 0.4494165662914207\n",
      "Iteration: 265000 Loss: 0.4494165662914207\n",
      "Iteration: 266000 Loss: 0.4494165662914207\n",
      "Iteration: 267000 Loss: 0.4494165662914207\n",
      "Iteration: 268000 Loss: 0.4494165662914207\n",
      "Iteration: 269000 Loss: 0.4494165662914207\n",
      "Iteration: 270000 Loss: 0.4494165662914207\n",
      "Iteration: 271000 Loss: 0.4494165662914207\n",
      "Iteration: 272000 Loss: 0.4494165662914207\n",
      "Iteration: 273000 Loss: 0.44941656629142074\n",
      "Iteration: 274000 Loss: 0.4494165662914207\n",
      "Iteration: 275000 Loss: 0.4494165662914207\n",
      "Iteration: 276000 Loss: 0.4494165662914207\n",
      "Iteration: 277000 Loss: 0.4494165662914207\n",
      "Iteration: 278000 Loss: 0.4494165662914207\n",
      "Iteration: 279000 Loss: 0.4494165662914207\n",
      "Iteration: 280000 Loss: 0.4494165662914207\n",
      "Iteration: 281000 Loss: 0.4494165662914207\n",
      "Iteration: 282000 Loss: 0.4494165662914207\n",
      "Iteration: 283000 Loss: 0.4494165662914207\n",
      "Iteration: 284000 Loss: 0.4494165662914207\n",
      "Iteration: 285000 Loss: 0.4494165662914207\n",
      "Iteration: 286000 Loss: 0.4494165662914207\n",
      "Iteration: 287000 Loss: 0.4494165662914207\n",
      "Iteration: 288000 Loss: 0.4494165662914207\n",
      "Iteration: 289000 Loss: 0.4494165662914207\n",
      "Iteration: 290000 Loss: 0.4494165662914207\n",
      "Iteration: 291000 Loss: 0.4494165662914207\n",
      "Iteration: 292000 Loss: 0.4494165662914207\n",
      "Iteration: 293000 Loss: 0.4494165662914207\n",
      "Iteration: 294000 Loss: 0.4494165662914207\n",
      "Iteration: 295000 Loss: 0.4494165662914207\n",
      "Iteration: 296000 Loss: 0.4494165662914207\n",
      "Iteration: 297000 Loss: 0.4494165662914207\n",
      "Iteration: 298000 Loss: 0.4494165662914207\n",
      "Iteration: 299000 Loss: 0.4494165662914207\n",
      "Iteration: 300000 Loss: 0.4494165662914207\n",
      "Iteration: 301000 Loss: 0.4494165662914207\n",
      "Iteration: 302000 Loss: 0.4494165662914207\n",
      "Iteration: 303000 Loss: 0.4494165662914207\n",
      "Iteration: 304000 Loss: 0.4494165662914207\n",
      "Iteration: 305000 Loss: 0.4494165662914207\n",
      "Iteration: 306000 Loss: 0.4494165662914207\n",
      "Iteration: 307000 Loss: 0.4494165662914207\n",
      "Iteration: 308000 Loss: 0.4494165662914207\n",
      "Iteration: 309000 Loss: 0.4494165662914207\n",
      "Iteration: 310000 Loss: 0.4494165662914207\n",
      "Iteration: 311000 Loss: 0.4494165662914207\n",
      "Iteration: 312000 Loss: 0.4494165662914207\n",
      "Iteration: 313000 Loss: 0.4494165662914207\n",
      "Iteration: 314000 Loss: 0.4494165662914207\n",
      "Iteration: 315000 Loss: 0.4494165662914207\n",
      "Iteration: 316000 Loss: 0.4494165662914207\n",
      "Iteration: 317000 Loss: 0.4494165662914207\n",
      "Iteration: 318000 Loss: 0.4494165662914206\n",
      "Iteration: 319000 Loss: 0.4494165662914207\n",
      "Iteration: 320000 Loss: 0.4494165662914207\n",
      "Iteration: 321000 Loss: 0.4494165662914207\n",
      "Iteration: 322000 Loss: 0.4494165662914207\n",
      "Iteration: 323000 Loss: 0.4494165662914207\n",
      "Iteration: 324000 Loss: 0.4494165662914207\n",
      "Iteration: 325000 Loss: 0.4494165662914207\n",
      "Iteration: 326000 Loss: 0.4494165662914207\n",
      "Iteration: 327000 Loss: 0.4494165662914207\n",
      "Iteration: 328000 Loss: 0.4494165662914207\n",
      "Iteration: 329000 Loss: 0.4494165662914207\n",
      "Iteration: 330000 Loss: 0.4494165662914207\n",
      "Iteration: 331000 Loss: 0.4494165662914207\n",
      "Iteration: 332000 Loss: 0.4494165662914207\n",
      "Iteration: 333000 Loss: 0.4494165662914207\n",
      "Iteration: 334000 Loss: 0.4494165662914207\n",
      "Iteration: 335000 Loss: 0.4494165662914207\n",
      "Iteration: 336000 Loss: 0.4494165662914207\n",
      "Iteration: 337000 Loss: 0.4494165662914207\n",
      "Iteration: 338000 Loss: 0.4494165662914207\n",
      "Iteration: 339000 Loss: 0.4494165662914207\n",
      "Iteration: 340000 Loss: 0.4494165662914207\n",
      "Iteration: 341000 Loss: 0.4494165662914207\n",
      "Iteration: 342000 Loss: 0.4494165662914207\n",
      "Iteration: 343000 Loss: 0.4494165662914207\n",
      "Iteration: 344000 Loss: 0.4494165662914207\n",
      "Iteration: 345000 Loss: 0.4494165662914207\n",
      "Iteration: 346000 Loss: 0.4494165662914207\n",
      "Iteration: 347000 Loss: 0.4494165662914207\n",
      "Iteration: 348000 Loss: 0.4494165662914207\n",
      "Iteration: 349000 Loss: 0.4494165662914207\n",
      "Iteration: 350000 Loss: 0.4494165662914207\n",
      "Iteration: 351000 Loss: 0.4494165662914207\n",
      "Iteration: 352000 Loss: 0.4494165662914207\n",
      "Iteration: 353000 Loss: 0.4494165662914207\n",
      "Iteration: 354000 Loss: 0.4494165662914207\n",
      "Iteration: 355000 Loss: 0.4494165662914207\n",
      "Iteration: 356000 Loss: 0.4494165662914207\n",
      "Iteration: 357000 Loss: 0.4494165662914207\n",
      "Iteration: 358000 Loss: 0.4494165662914207\n",
      "Iteration: 359000 Loss: 0.4494165662914207\n",
      "Iteration: 360000 Loss: 0.4494165662914207\n",
      "Iteration: 361000 Loss: 0.4494165662914207\n",
      "Iteration: 362000 Loss: 0.4494165662914207\n",
      "Iteration: 363000 Loss: 0.4494165662914207\n",
      "Iteration: 364000 Loss: 0.4494165662914207\n",
      "Iteration: 365000 Loss: 0.4494165662914207\n",
      "Iteration: 366000 Loss: 0.4494165662914207\n",
      "Iteration: 367000 Loss: 0.4494165662914207\n",
      "Iteration: 368000 Loss: 0.4494165662914207\n",
      "Iteration: 369000 Loss: 0.4494165662914207\n",
      "Iteration: 370000 Loss: 0.4494165662914207\n",
      "Iteration: 371000 Loss: 0.4494165662914207\n",
      "Iteration: 372000 Loss: 0.4494165662914207\n",
      "Iteration: 373000 Loss: 0.4494165662914207\n",
      "Iteration: 374000 Loss: 0.4494165662914207\n",
      "Iteration: 375000 Loss: 0.4494165662914207\n",
      "Iteration: 376000 Loss: 0.4494165662914207\n",
      "Iteration: 377000 Loss: 0.4494165662914207\n",
      "Iteration: 378000 Loss: 0.4494165662914207\n",
      "Iteration: 379000 Loss: 0.4494165662914207\n",
      "Iteration: 380000 Loss: 0.4494165662914207\n",
      "Iteration: 381000 Loss: 0.4494165662914207\n",
      "Iteration: 382000 Loss: 0.4494165662914207\n",
      "Iteration: 383000 Loss: 0.4494165662914207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 384000 Loss: 0.4494165662914207\n",
      "Iteration: 385000 Loss: 0.4494165662914207\n",
      "Iteration: 386000 Loss: 0.4494165662914207\n",
      "Iteration: 387000 Loss: 0.4494165662914207\n",
      "Iteration: 388000 Loss: 0.4494165662914207\n",
      "Iteration: 389000 Loss: 0.4494165662914207\n",
      "Iteration: 390000 Loss: 0.4494165662914207\n",
      "Iteration: 391000 Loss: 0.4494165662914207\n",
      "Iteration: 392000 Loss: 0.4494165662914207\n",
      "Iteration: 393000 Loss: 0.4494165662914207\n",
      "Iteration: 394000 Loss: 0.4494165662914207\n",
      "Iteration: 395000 Loss: 0.4494165662914207\n",
      "Iteration: 396000 Loss: 0.4494165662914207\n",
      "Iteration: 397000 Loss: 0.4494165662914207\n",
      "Iteration: 398000 Loss: 0.4494165662914207\n",
      "Iteration: 399000 Loss: 0.4494165662914207\n",
      "Iteration: 400000 Loss: 0.4494165662914207\n",
      "Iteration: 401000 Loss: 0.4494165662914207\n",
      "Iteration: 402000 Loss: 0.4494165662914207\n",
      "Iteration: 403000 Loss: 0.4494165662914207\n",
      "Iteration: 404000 Loss: 0.4494165662914207\n",
      "Iteration: 405000 Loss: 0.4494165662914207\n",
      "Iteration: 406000 Loss: 0.4494165662914207\n",
      "Iteration: 407000 Loss: 0.4494165662914207\n",
      "Iteration: 408000 Loss: 0.4494165662914207\n",
      "Iteration: 409000 Loss: 0.4494165662914207\n",
      "Iteration: 410000 Loss: 0.4494165662914207\n",
      "Iteration: 411000 Loss: 0.4494165662914207\n",
      "Iteration: 412000 Loss: 0.4494165662914207\n",
      "Iteration: 413000 Loss: 0.4494165662914207\n",
      "Iteration: 414000 Loss: 0.4494165662914207\n",
      "Iteration: 415000 Loss: 0.4494165662914207\n",
      "Iteration: 416000 Loss: 0.4494165662914207\n",
      "Iteration: 417000 Loss: 0.4494165662914207\n",
      "Iteration: 418000 Loss: 0.4494165662914207\n",
      "Iteration: 419000 Loss: 0.4494165662914207\n",
      "Iteration: 420000 Loss: 0.4494165662914207\n",
      "Iteration: 421000 Loss: 0.4494165662914207\n",
      "Iteration: 422000 Loss: 0.4494165662914207\n",
      "Iteration: 423000 Loss: 0.4494165662914207\n",
      "Iteration: 424000 Loss: 0.4494165662914207\n",
      "Iteration: 425000 Loss: 0.4494165662914207\n",
      "Iteration: 426000 Loss: 0.4494165662914207\n",
      "Iteration: 427000 Loss: 0.4494165662914207\n",
      "Iteration: 428000 Loss: 0.4494165662914207\n",
      "Iteration: 429000 Loss: 0.4494165662914207\n",
      "Iteration: 430000 Loss: 0.4494165662914207\n",
      "Iteration: 431000 Loss: 0.4494165662914207\n",
      "Iteration: 432000 Loss: 0.4494165662914207\n",
      "Iteration: 433000 Loss: 0.4494165662914207\n",
      "Iteration: 434000 Loss: 0.4494165662914207\n",
      "Iteration: 435000 Loss: 0.4494165662914207\n",
      "Iteration: 436000 Loss: 0.4494165662914207\n",
      "Iteration: 437000 Loss: 0.4494165662914207\n",
      "Iteration: 438000 Loss: 0.4494165662914207\n",
      "Iteration: 439000 Loss: 0.4494165662914207\n",
      "Iteration: 440000 Loss: 0.4494165662914207\n",
      "Iteration: 441000 Loss: 0.4494165662914207\n",
      "Iteration: 442000 Loss: 0.4494165662914207\n",
      "Iteration: 443000 Loss: 0.4494165662914207\n",
      "Iteration: 444000 Loss: 0.4494165662914207\n",
      "Iteration: 445000 Loss: 0.4494165662914207\n",
      "Iteration: 446000 Loss: 0.4494165662914207\n",
      "Iteration: 447000 Loss: 0.4494165662914207\n",
      "Iteration: 448000 Loss: 0.4494165662914207\n",
      "Iteration: 449000 Loss: 0.4494165662914207\n",
      "Iteration: 450000 Loss: 0.4494165662914207\n",
      "Iteration: 451000 Loss: 0.4494165662914207\n",
      "Iteration: 452000 Loss: 0.4494165662914207\n",
      "Iteration: 453000 Loss: 0.4494165662914207\n",
      "Iteration: 454000 Loss: 0.4494165662914207\n",
      "Iteration: 455000 Loss: 0.4494165662914207\n",
      "Iteration: 456000 Loss: 0.4494165662914207\n",
      "Iteration: 457000 Loss: 0.4494165662914207\n",
      "Iteration: 458000 Loss: 0.4494165662914207\n",
      "Iteration: 459000 Loss: 0.4494165662914207\n",
      "Iteration: 460000 Loss: 0.4494165662914207\n",
      "Iteration: 461000 Loss: 0.4494165662914207\n",
      "Iteration: 462000 Loss: 0.4494165662914207\n",
      "Iteration: 463000 Loss: 0.4494165662914207\n",
      "Iteration: 464000 Loss: 0.4494165662914207\n",
      "Iteration: 465000 Loss: 0.4494165662914207\n",
      "Iteration: 466000 Loss: 0.4494165662914207\n",
      "Iteration: 467000 Loss: 0.4494165662914207\n",
      "Iteration: 468000 Loss: 0.4494165662914207\n",
      "Iteration: 469000 Loss: 0.4494165662914207\n",
      "Iteration: 470000 Loss: 0.4494165662914207\n",
      "Iteration: 471000 Loss: 0.4494165662914207\n",
      "Iteration: 472000 Loss: 0.4494165662914207\n",
      "Iteration: 473000 Loss: 0.4494165662914207\n",
      "Iteration: 474000 Loss: 0.4494165662914207\n",
      "Iteration: 475000 Loss: 0.4494165662914207\n",
      "Iteration: 476000 Loss: 0.4494165662914207\n",
      "Iteration: 477000 Loss: 0.4494165662914207\n",
      "Iteration: 478000 Loss: 0.4494165662914207\n",
      "Iteration: 479000 Loss: 0.4494165662914207\n",
      "Iteration: 480000 Loss: 0.4494165662914207\n",
      "Iteration: 481000 Loss: 0.4494165662914207\n",
      "Iteration: 482000 Loss: 0.4494165662914207\n",
      "Iteration: 483000 Loss: 0.4494165662914207\n",
      "Iteration: 484000 Loss: 0.4494165662914207\n",
      "Iteration: 485000 Loss: 0.4494165662914207\n",
      "Iteration: 486000 Loss: 0.4494165662914207\n",
      "Iteration: 487000 Loss: 0.4494165662914207\n",
      "Iteration: 488000 Loss: 0.4494165662914207\n",
      "Iteration: 489000 Loss: 0.4494165662914207\n",
      "Iteration: 490000 Loss: 0.4494165662914207\n",
      "Iteration: 491000 Loss: 0.4494165662914207\n",
      "Iteration: 492000 Loss: 0.4494165662914207\n",
      "Iteration: 493000 Loss: 0.4494165662914207\n",
      "Iteration: 494000 Loss: 0.4494165662914207\n",
      "Iteration: 495000 Loss: 0.4494165662914207\n",
      "Iteration: 496000 Loss: 0.4494165662914207\n",
      "Iteration: 497000 Loss: 0.4494165662914207\n",
      "Iteration: 498000 Loss: 0.4494165662914207\n",
      "Iteration: 499000 Loss: 0.4494165662914207\n",
      "Iteration: 500000 Loss: 0.4494165662914207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialise_params(no_features):\n",
    "    W = np.random.randn(no_features, 1)\n",
    "    B = np.random.randn(1, 1)\n",
    "    return W, B\n",
    "\n",
    "def forward_prop(W, B, X):\n",
    "    z = np.dot(X, W) + B\n",
    "    return z\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def threshold(A):\n",
    "    return np.where(A >= 0.5, 1, 0)\n",
    "\n",
    "def bin_cross_entropy_loss(y_pred, Y_train):\n",
    "    epsilon = 1e-15\n",
    "    loss = (-1/len(Y_train)) * np.sum(Y_train * np.log(y_pred + epsilon) + (1 - Y_train) * np.log(1 - y_pred + epsilon))\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X_train, Y_train, A, W, B, alpha, max_grad_norm):\n",
    "    m = len(Y_train)\n",
    "    dw = (1/m) * np.dot(X_train.T, (A - Y_train))\n",
    "    db = np.sum(A - Y_train) / m\n",
    "    \n",
    "    # Clip gradients using max_grad_norm\n",
    "    max_norm = np.linalg.norm(dw, ord=2)  # Calculate L2 norm\n",
    "    if max_norm > max_grad_norm:\n",
    "        dw = dw * (max_grad_norm / max_norm)\n",
    "    \n",
    "    W = W - alpha * dw\n",
    "    B = B - alpha * db\n",
    "    \n",
    "    return W, B\n",
    "\n",
    "def logistic_regression_with_plot(no_iterations, X_train, Y_train, X_test, Y_test):\n",
    "    no_samples, no_features = X_train.shape\n",
    "    W, B = initialise_params(no_features)\n",
    "    \n",
    "    losses_train = []  # Store losses for training set\n",
    "    losses_test = []   # Store losses for test set\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(no_iterations):\n",
    "        Z_train = forward_prop(W, B, X_train)\n",
    "        A_train = sigmoid(Z_train)\n",
    "        loss_train = bin_cross_entropy_loss(A_train, Y_train)\n",
    "        losses_train.append(loss_train)\n",
    "        \n",
    "        Z_test = forward_prop(W, B, X_test)\n",
    "        A_test = sigmoid(Z_test)\n",
    "        loss_test = bin_cross_entropy_loss(A_test, Y_test)\n",
    "        losses_test.append(loss_test)\n",
    "        \n",
    "        W, B = gradient_descent(X_train, Y_train, A_train, W, B, 0.1, max_grad_norm=1.0)\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"Iteration:\", i + 1, \"Loss:\", loss_train)\n",
    "    \n",
    "    y_pred = threshold(A_train)\n",
    "    \n",
    "    # Plot the loss curves for training and test sets\n",
    "    plt.plot(losses_train, label=\"Training Set Loss\")\n",
    "    plt.plot(losses_test, label=\"Test Set Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "Y_train = Y_train.values.reshape(-1, 1)\n",
    "Y_test = Y_test.values.reshape(-1, 1)\n",
    "\n",
    "y_pred = logistic_regression_with_plot(500000, X_train, Y_train, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e00e8e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800561797752809\n",
      "0.752\n",
      "0.7014925373134329\n",
      "0.7258687258687259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the model using different metrics\n",
    "accuracy = accuracy_score(Y_train, y_pred)\n",
    "precision = precision_score(Y_train, y_pred)\n",
    "recall = recall_score(Y_train, y_pred)\n",
    "f1 = f1_score(Y_train, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5c6ba",
   "metadata": {},
   "source": [
    "The model appears to flatten to a loss of 0.45 with no clear evidence of overfitting. The data set is relatively small, accuracy is 80%, which isn't ideal. f1 score is 0.73, which is ok. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
