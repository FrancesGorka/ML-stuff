{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac30a78b-aa00-4ae9-b682-1aba0b9b0063",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\"> Multi-label Classification of enzyme substrates </span> \n",
    "\n",
    "Key skills covered: Multilabel classification, Random Forests, bagging, data cleaning methods (drop, concatenate, splitting columns, dropping empty values), dealing with imbalanced class sizes (weighting of classes, oversampling), hyperparameter tuning with grid search.\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Dataset </span> \n",
    "\n",
    "There are 3 files names mixed_(desc, ecfp, fcfp).csv containing chemical, structural, connectivity information. Each csv list all samples with a unique ID, with a different collection of variables in the columns and the final column specifying which classes they're in as follows: 1_1_1_1_0_1. \n",
    "\n",
    "<span style=\"font-size: 20px;\"> Problem space </span> \n",
    "\n",
    "First we combine them into a single dataset.\n",
    "\n",
    "We have a dataset of substrates, and a range of features. We want to predict the EC classes of the substrates. We have six classes, and then can belong to multiple, since the same molecules participate in different types of reactions.\n",
    "\n",
    "The dataset is highly imbalanced in labels (smallest is 1, highest is 248), so we need to opt for an algorithm that can tackle label imbalance. Note that we don't have to scale the features as this doesn't matter for tree based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895d4f9c-3234-4f13-a6bd-36caa887ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro, zscore\n",
    "path_desc = \"/home/frances/Documents/Getting a JOB/ML preparation/Multilabel_Substrate_Classification/mixed_desc.csv\"\n",
    "path_ecfp = \"/home/frances/Documents/Getting a JOB/ML preparation/Multilabel_Substrate_Classification/mixed_ecfp.csv\"\n",
    "path_fcfp = \"/home/frances/Documents/Getting a JOB/ML preparation/Multilabel_Substrate_Classification/mixed_fcfp.csv\"\n",
    "\n",
    "desc = pd.read_csv(path_desc)\n",
    "desc.drop(['EC1_EC2_EC3_EC4_EC5_EC6'],axis=1,inplace=True)\n",
    "ecfp = pd.read_csv(path_ecfp)\n",
    "ecfp.drop(['EC1_EC2_EC3_EC4_EC5_EC6'],axis=1,inplace=True)\n",
    "fcfp = pd.read_csv(path_fcfp)\n",
    "\n",
    "merged_data = pd.concat([desc, ecfp, fcfp], axis=1) # concaternating files\n",
    "merged_data.drop(['CIDs'],axis=1,inplace=True)\n",
    "\n",
    "data = {'EC1_EC2_EC3_EC4_EC5_EC6': merged_data['EC1_EC2_EC3_EC4_EC5_EC6']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the 'Column' values by '_' and expand into separate columns\n",
    "df[['EC1', 'EC2', 'EC3', 'EC4', 'EC5', 'EC6']] = df['EC1_EC2_EC3_EC4_EC5_EC6'].str.split('_', expand=True).astype(int)\n",
    "\n",
    "# Drop the original 'Column' after extracting the EC values\n",
    "df.drop(columns=['EC1_EC2_EC3_EC4_EC5_EC6'], inplace=True)\n",
    "\n",
    "merged_data = merged_data.drop('EC1_EC2_EC3_EC4_EC5_EC6',axis=1)\n",
    "#cleaned_data = pd.concat([merged_data, df], axis=1)\n",
    "#cleaned_data = cleaned_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15fd02ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2106664645.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    for column in columns_to normalise:\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "columns_to_normalise = []\n",
    "# Loop through each column\n",
    "for column_name in merged_data.columns:\n",
    "    # Count null values\n",
    "    null_count = merged_data[column_name].isnull().sum()\n",
    "    print(f\"Null Count: {null_count}\")\n",
    "    statistic, p_value = shapiro(merged_data[column_name].dropna())\n",
    "    if p_value<0.05:\n",
    "        columns_to_normalise.append(column_name)\n",
    "\n",
    "print(columns_to_normalise)\n",
    "\n",
    "for column in columns_to normalise:\n",
    "    merged_data[column] = np.log(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Z-Score and Outlier Detection\n",
    "        z_scores = zscore(merged_data[column_name].dropna())\n",
    "        threshold = 3\n",
    "        outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "        print(\"Outliers detected:\")\n",
    "        print(outliers)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2ba34-5027-439f-9684-8b3b28f00ece",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> Random Forest </span> \n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees. It can handle imbalanced datasets well by averaging predictions from different trees, which reduces the risk of overfitting to the majority class. It is also less sensitive to outliers and can manage high dimensional data well. Random forests also can be used for feature selection since they provide a metric of importance of features. \n",
    "\n",
    "<b> How it works:</b>\n",
    "\n",
    "Random Forest uses a few techniques:\n",
    "- \"Bagging\" (Bootstrap Aggregating) is used to create multiple decision trees. Bootstrapping means a random sample is drawn with replacement from the original data to train an individual decision tree.\n",
    "- Random Feature Selection is used to introduce randomness by only selecting a random subset of features at each branch\n",
    "- For classification and regression tasks, each tree will provide a prediction. For classification, the final result is the most frequent prediction. For regression, it's an average of the numerical values predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "10693358-04ae-49ea-9dc5-a3bfc7779222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation dataset: 0.19230769230769232\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = cleaned_data.drop(['EC1','EC2','EC3','EC4','EC5','EC6'], axis=1)\n",
    "Y = df\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=42) \n",
    "X_eval, X_test, Y_eval, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier() # initialise the model\n",
    "\n",
    "X_train['Ipc'] = X_train['Ipc'].astype(int) # had an issue with the values being too large for float\n",
    "\n",
    "model.fit(X_train,Y_train) # fit the model\n",
    "y_pred_eval = model.predict(X_eval) # make predictions on evaluation data\n",
    "accuracy_eval = accuracy_score(Y_eval, y_pred_eval)  \n",
    "\n",
    "print(f\"Accuracy on the validation dataset: {accuracy_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dede101-c0de-4959-ad3d-0a6ed0e7ba54",
   "metadata": {},
   "source": [
    "It's clear that the accuracy is incredibly low, likely due to the highly inbalanced class sizes. Let's make use of weightings to see if we can change this.\n",
    "\n",
    "We'll use a balanced weighting first and then adjust by class sizes, to see if it leads to further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "15e4053c-e1df-47f7-b9bf-8e22d79a00a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation dataset with weight balancing: 0.21634615384615385\n"
     ]
    }
   ],
   "source": [
    "EC1_no = Y_train['EC1'].value_counts()\n",
    "EC2_no = Y_train['EC2'].value_counts()\n",
    "EC3_no = Y_train['EC3'].value_counts()\n",
    "EC4_no = Y_train['EC4'].value_counts()\n",
    "EC5_no = Y_train['EC5'].value_counts()\n",
    "EC6_no = Y_train['EC6'].value_counts()\n",
    "\n",
    "class_counts_asc = [{0: EC1_no[0], 1: EC1_no[1]},\n",
    "                    {0: EC2_no[0], 1: EC2_no[1]},\n",
    "                    {0: EC3_no[0], 1: EC3_no[1]},\n",
    "                    {0: EC4_no[0], 1: EC4_no[1]},\n",
    "                    {0: EC5_no[0], 1: EC5_no[1]},\n",
    "                    {0: EC6_no[0], 1: EC6_no[1]}]\n",
    "\n",
    "model_bal = RandomForestClassifier(class_weight=class_counts_asc)\n",
    "\n",
    "model_bal.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_eval_bal = model_bal.predict(X_eval) # make predictions on evaluation data\n",
    "accuracy_eval_bal = accuracy_score(Y_eval, y_pred_eval_bal)  \n",
    "\n",
    "print(f\"Accuracy on the validation dataset with weight balancing: {accuracy_eval_bal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fcaf65-acf7-4ae9-a9ff-14f8df06922c",
   "metadata": {},
   "source": [
    "The accuracy is barely different. It could be that I did something weird when I was cleaning the data. There are a lot of input features, which can lead to overfitting, but usually Random Forest is a method that is well equipped to deal with this. If that is the problem, then hyperparameter tuning should help matters. \n",
    "\n",
    "<b> Problems </b>\n",
    "\n",
    "I would like to look at the confusion_matrix, f1_score, precision_score, recall_score and I'd like to perform oversampling with the imbalance learn library (imblearn.oversampling import SMOTE). HOWEVER, none of these methods support multi-label classification. You have to basically compute them on the classes separately and then amalgamate the results. I don't have time for that now, so we'll have to give up.\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Hyperparameter Tuning </span> \n",
    "\n",
    "The general approach to Hyperparameter Tuning is to employ grid search or random search. Grid search involves defining a dictionary of hyperparameter values to explore. The key is the hyperparameter, the value is a list of different values. You then create an instance of the GridSearchCV class, with input parameters as follows:\n",
    "- estimator = model (the initiated instance of the RandomForestClassifier class)\n",
    "- paramgrid=paramgrid (the dictionary of hyperparameters)\n",
    "- cv=5 (cross validation value, usually 5 or 10)\n",
    "\n",
    "Then we do grid_search.fit(X_train,Y_train) to fit the training data.\n",
    "\n",
    "To access the best hyperparameters (values that yielded the best performance) and best estimator (the model object tuned with the best hyperparameters), we use grid_search.best_params_ and grid_search.best_estimator_.\n",
    "\n",
    "Grid search is a bit computationally expensive, instead you can choose randomly and optimise. There are various methods, like random search or bayesian optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8f74e351-5110-4bcc-972d-1298b0889987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20192307692307693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # number of trees\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model_bal, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV on training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Access best hyperparameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "accuracy = best_model.score(X_eval, Y_eval)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322c33c-4106-4ddd-b9cf-698b93cb0a9a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> Conclusion </span> \n",
    "\n",
    "Well, that was a fail. If I had more time I would have re-pre-processed the data. It would help to have an understanding of the data, but I'm not a biology expert. I would also look at some of the other evaluation methods, and performed oversampling. I would then opt for trying another algorithm, like XG Boost, Support Vector Machines, and maybe even a deep neural net.\n",
    "\n",
    "<span style=\"font-size: 20px;\"> Next steps </span> \n",
    "\n",
    "Next I will switch to regression, and try to find a neater dataset so that doesn't detract from the model building. I'll spend some time properly on feature engineering and data cleaning at some point. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
