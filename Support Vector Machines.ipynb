{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2154c5-a626-4b11-9c2b-0166af2eb261",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\"> Support Vector Machines </span> \n",
    "\n",
    "SVM is a binary classifier.\n",
    "\n",
    "Step 1) A hyperplane is represented by the equation w * x + b = 0, where w is the weight vector and b is the bias. This is known as the decision boundary. We classify points with 1 if w⋅x+b ≥ 0 and -1 if w⋅x+b < 0 by convention. This is because (since the hyperplane lies equidistant) we can make w * x + b = 0. So w * x is a vector passing through the origin, and w * x + b = 0 passes through -b (shifted). w * x + b = 1 and w * x + b = -1 are parallel but shifted +-1 in either direction. \n",
    "\n",
    "Note that w is a gradient, and support vectors are orthogonal vectors (points) which, if removed, completely change the hyperplane position. Support vectors are within a distance d of the hyperplane. \n",
    "\n",
    "Note that because y = -1 if w⋅x+b < 0 i.e. w⋅x+b =< -1, then y(i)x(w⋅x+b) => 1. And if y = 1 then y(i)x(w⋅x+b > 0 i.e. y(i)x(w⋅x+b) => 1. So it's always the case that y(i)x(w⋅x+b) => 1.\n",
    "\n",
    "Step 2) The margin refers to a separation distance, i.e. the closest data points to that plane are found, and then their distance, and then this is summed. A small value would likely be an overfit model, as the plane would be more influenced by noise and outliers. Also, the larger this is, the clearer the separation between classes.\n",
    "\n",
    "Step 3) We have an optimisation function. We want to minimise the error and maximise the margin. \n",
    "\n",
    "Note that points on the line w * x + b = -1 have distance -1-b/|w| and points on the line w * x + b = 1 have distance 1-b/|w|. For whatever reason (figure out later), you subtract the distance of the + hyperplane with the distance of the - hyperplane, to get the 2/|w|, twice the margin. So we want to maximise 1/|w|, which is the same thing as maximising |w| with the contraint y(i)x(w⋅x+b) => 1 \n",
    "\n",
    "Note that if we were to just use this as the optimisation function, we'd be making an assumption of linear separation. Most often that isn't the case, the feature vectors aren't linearly separable. In this case, we apply the kernel trick. This involves mapping the original data points from the lower-dimensional space to a higher-dimensional space using a mathematical function called a kernel. This mapping can be implicit and doesn't require explicitly calculating the coordinates in the higher-dimensional space.\n",
    "\n",
    "<b> Let's move on to implementation </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39622cf-cca4-4f78-9ca2-d08a606af4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC # SVC is the suppport vector classifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Suppose we've already loaded X_train, Y_train. \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
